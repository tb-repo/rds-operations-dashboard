# Testing Guide for RDS Operations Dashboard

**Generated by:** claude-3.5-sonnet  
**Timestamp:** 2025-11-12T19:30:00Z  
**Version:** 1.0.0

## Overview

This guide covers multiple testing approaches for the RDS Operations Dashboard, from local unit tests to deployed integration tests.

## Testing Levels

1. **Unit Tests** - Test individual functions with mocked AWS services
2. **Integration Tests** - Test with real AWS services in test environment
3. **End-to-End Tests** - Test complete workflows after deployment
4. **Manual Testing** - Interactive testing via AWS Console/CLI

---

## 1. Unit Tests (Local)

### Setup

```bash
# Install testing dependencies
cd rds-operations-dashboard/lambda
pip install pytest pytest-cov moto boto3

# Install project dependencies
pip install -r requirements.txt
```

### Create Unit Tests

Create `lambda/tests/test_discovery.py`:

```python
import pytest
import json
from moto import mock_dynamodb, mock_rds, mock_sts
import boto3
from datetime import datetime

# Mock AWS services
@mock_dynamodb
@mock_rds
@mock_sts
def test_discovery_handler():
    """Test discovery Lambda handler with mocked AWS services."""
    
    # Setup mock DynamoDB table
    dynamodb = boto3.resource('dynamodb', region_name='ap-southeast-1')
    table = dynamodb.create_table(
        TableName='rds-inventory-test',
        KeySchema=[{'AttributeName': 'instance_id', 'KeyType': 'HASH'}],
        AttributeDefinitions=[{'AttributeName': 'instance_id', 'AttributeType': 'S'}],
        BillingMode='PAY_PER_REQUEST'
    )
    
    # Setup mock RDS instance
    rds = boto3.client('rds', region_name='ap-southeast-1')
    rds.create_db_instance(
        DBInstanceIdentifier='test-instance-1',
        DBInstanceClass='db.t3.micro',
        Engine='postgres',
        MasterUsername='admin',
        MasterUserPassword='password123'
    )
    
    # Set environment variables
    import os
    os.environ['INVENTORY_TABLE'] = 'rds-inventory-test'
    os.environ['METRICS_CACHE_TABLE'] = 'metrics-cache-test'
    os.environ['HEALTH_ALERTS_TABLE'] = 'health-alerts-test'
    os.environ['AUDIT_LOG_TABLE'] = 'audit-log-test'
    os.environ['DATA_BUCKET'] = 'test-bucket'
    os.environ['EXTERNAL_ID'] = 'test-id'
    os.environ['TARGET_ACCOUNTS'] = '["123456789012"]'
    os.environ['TARGET_REGIONS'] = '["ap-southeast-1"]'
    os.environ['SNS_TOPIC_ARN'] = 'arn:aws:sns:ap-southeast-1:123456789012:test'
    
    # Import and test handler
    from discovery.handler import lambda_handler
    
    class MockContext:
        request_id = 'test-123'
        function_name = 'test-function'
    
    result = lambda_handler({}, MockContext())
    
    # Assertions
    assert result['statusCode'] == 200
    body = json.loads(result['body'])
    assert body['total_instances'] >= 1
    assert 'persistence' in body


@mock_dynamodb
def test_cache_manager():
    """Test metrics cache manager."""
    from health_monitor.cache_manager import MetricsCacheManager
    
    # Setup mock table
    dynamodb = boto3.resource('dynamodb', region_name='ap-southeast-1')
    table = dynamodb.create_table(
        TableName='metrics-cache-test',
        KeySchema=[{'AttributeName': 'cache_key', 'KeyType': 'HASH'}],
        AttributeDefinitions=[{'AttributeName': 'cache_key', 'AttributeType': 'S'}],
        BillingMode='PAY_PER_REQUEST'
    )
    
    # Mock config
    class MockConfig:
        class dynamodb:
            metrics_cache_table = 'metrics-cache-test'
        class cache:
            metrics_ttl_seconds = 300
        class monitoring:
            cloudwatch_namespace = 'Test'
    
    # Test cache manager
    cache = MetricsCacheManager(MockConfig())
    
    # Test put
    success = cache.put(
        instance_id='test-instance',
        metric_name='CPUUtilization',
        period=300,
        value=45.5,
        timestamp=datetime.utcnow(),
        unit='Percent'
    )
    assert success == True
    
    # Test get
    cached = cache.get('test-instance', 'CPUUtilization', 300)
    assert cached is not None
    assert cached['value'] == 45.5
    
    # Test statistics
    stats = cache.get_statistics()
    assert stats['cache_hits'] == 1
    assert stats['cache_writes'] == 1
```

### Run Unit Tests

```bash
cd lambda

# Run all tests
pytest tests/ -v

# Run with coverage
pytest tests/ --cov=. --cov-report=html

# Run specific test
pytest tests/test_discovery.py::test_discovery_handler -v

# View coverage report
open htmlcov/index.html  # macOS
# or
start htmlcov/index.html  # Windows
```

---

## 2. Local Testing with AWS LocalStack

### Setup LocalStack

```bash
# Install LocalStack
pip install localstack awscli-local

# Start LocalStack
localstack start

# Or with Docker
docker run -d -p 4566:4566 localstack/localstack
```

### Test with LocalStack

```bash
# Create test DynamoDB table
awslocal dynamodb create-table \
  --table-name rds-inventory-test \
  --attribute-definitions AttributeName=instance_id,AttributeType=S \
  --key-schema AttributeName=instance_id,KeyType=HASH \
  --billing-mode PAY_PER_REQUEST

# Set environment to use LocalStack
export AWS_ENDPOINT_URL=http://localhost:4566

# Run Lambda locally
python lambda/discovery/handler.py
```

---

## 3. Integration Testing (AWS Test Environment)

### Prerequisites

```bash
# Deploy to test environment
cd infrastructure
cdk deploy --all -c environment=test

# Get deployed resources
aws cloudformation describe-stacks \
  --stack-name RDSDashboard-Data-test \
  --query 'Stacks[0].Outputs'
```

### Test Discovery Lambda

```bash
# Invoke Lambda function
aws lambda invoke \
  --function-name rds-discovery-test \
  --payload '{}' \
  --log-type Tail \
  response.json

# View response
cat response.json | jq .

# Check CloudWatch Logs
aws logs tail /aws/lambda/rds-discovery-test --follow

# Check DynamoDB for discovered instances
aws dynamodb scan \
  --table-name rds-inventory-test \
  --max-items 5
```

### Test Health Monitor Lambda

```bash
# First, ensure discovery has run and populated inventory
aws lambda invoke \
  --function-name rds-discovery-test \
  --payload '{}' \
  response.json

# Then test health monitor
aws lambda invoke \
  --function-name rds-health-monitor-test \
  --payload '{}' \
  --log-type Tail \
  response.json

# View response
cat response.json | jq .

# Check metrics cache
aws dynamodb scan \
  --table-name metrics-cache-test \
  --max-items 10

# Check CloudWatch metrics
aws cloudwatch list-metrics \
  --namespace DBMRDSDashboard

# Get specific metric
aws cloudwatch get-metric-statistics \
  --namespace DBMRDSDashboard \
  --metric-name CacheHitRate \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 300 \
  --statistics Average
```

---

## 4. Manual Testing via AWS Console

### Test Discovery Service

1. **Navigate to Lambda Console**
   - Go to AWS Lambda console
   - Find function: `rds-discovery-prod`

2. **Create Test Event**
   ```json
   {
     "test": true
   }
   ```

3. **Invoke Function**
   - Click "Test" button
   - Wait for execution to complete
   - Review execution results

4. **Verify Results**
   - Check CloudWatch Logs for execution details
   - Go to DynamoDB console
   - Open `rds-inventory-prod` table
   - Verify instances are discovered

### Test Health Monitor Service

1. **Ensure Inventory Exists**
   - Run discovery first (above)
   - Verify instances in DynamoDB

2. **Invoke Health Monitor**
   - Go to `rds-health-monitor-prod` Lambda
   - Click "Test"
   - Review results

3. **Check Cache**
   - Go to DynamoDB console
   - Open `metrics-cache-prod` table
   - Verify metrics are cached
   - Check TTL values

4. **View CloudWatch Metrics**
   - Go to CloudWatch console
   - Select "Metrics"
   - Choose "DBMRDSDashboard" namespace
   - View cache hit rate, instances monitored, etc.

---

## 5. Python Diagnostics Check

```bash
# Check Python syntax
cd lambda
python -m py_compile discovery/handler.py
python -m py_compile health-monitor/handler.py
python -m py_compile shared/*.py

# Check for import errors
python -c "from discovery import handler; print('Discovery: OK')"
python -c "from health_monitor import handler; print('Health Monitor: OK')"
python -c "from shared import config; print('Config: OK')"

# Run pylint (if installed)
pip install pylint
pylint discovery/handler.py
pylint health-monitor/handler.py
```

---

## 6. TypeScript/CDK Testing

```bash
cd infrastructure

# Install dependencies
npm install

# Compile TypeScript
npm run build

# Check for errors
tsc --noEmit

# Synthesize CloudFormation
cdk synth

# Validate templates
cdk diff

# Run CDK tests (if you create them)
npm test
```

---

## 7. End-to-End Testing Script

Create `test-e2e.sh`:

```bash
#!/bin/bash
set -e

echo "=== RDS Dashboard End-to-End Test ==="

# 1. Test Discovery
echo "1. Testing Discovery Service..."
aws lambda invoke \
  --function-name rds-discovery-prod \
  --payload '{}' \
  discovery-response.json

INSTANCES=$(cat discovery-response.json | jq -r '.body' | jq -r '.total_instances')
echo "   ✓ Discovered $INSTANCES instances"

# 2. Verify DynamoDB
echo "2. Verifying DynamoDB inventory..."
COUNT=$(aws dynamodb scan \
  --table-name rds-inventory-prod \
  --select COUNT \
  --query 'Count' \
  --output text)
echo "   ✓ Found $COUNT instances in DynamoDB"

# 3. Test Health Monitor
echo "3. Testing Health Monitor Service..."
aws lambda invoke \
  --function-name rds-health-monitor-prod \
  --payload '{}' \
  health-response.json

MONITORED=$(cat health-response.json | jq -r '.body' | jq -r '.instances_monitored')
CACHE_RATE=$(cat health-response.json | jq -r '.body' | jq -r '.cache_hit_rate')
echo "   ✓ Monitored $MONITORED instances"
echo "   ✓ Cache hit rate: $CACHE_RATE%"

# 4. Check CloudWatch Metrics
echo "4. Checking CloudWatch Metrics..."
METRICS=$(aws cloudwatch list-metrics \
  --namespace DBMRDSDashboard \
  --query 'length(Metrics)' \
  --output text)
echo "   ✓ Found $METRICS metrics in CloudWatch"

# 5. Check Logs
echo "5. Checking CloudWatch Logs..."
aws logs tail /aws/lambda/rds-discovery-prod --since 5m | head -20
aws logs tail /aws/lambda/rds-health-monitor-prod --since 5m | head -20

echo ""
echo "=== All Tests Passed! ==="
```

Run it:
```bash
chmod +x test-e2e.sh
./test-e2e.sh
```

---

## 8. Performance Testing

### Test Discovery Performance

```bash
# Time the discovery
time aws lambda invoke \
  --function-name rds-discovery-prod \
  --payload '{}' \
  response.json

# Check execution duration
aws cloudwatch get-metric-statistics \
  --namespace AWS/Lambda \
  --metric-name Duration \
  --dimensions Name=FunctionName,Value=rds-discovery-prod \
  --start-time $(date -u -d '1 hour ago' +%Y-%m-%dT%H:%M:%S) \
  --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \
  --period 300 \
  --statistics Average,Maximum
```

### Test Cache Performance

```bash
# First run (cache miss)
time aws lambda invoke \
  --function-name rds-health-monitor-prod \
  --payload '{}' \
  response1.json

# Second run (cache hit)
time aws lambda invoke \
  --function-name rds-health-monitor-prod \
  --payload '{}' \
  response2.json

# Compare cache hit rates
echo "First run:"
cat response1.json | jq '.body' | jq '.cache_hit_rate'

echo "Second run:"
cat response2.json | jq '.body' | jq '.cache_hit_rate'
```

---

## 9. Troubleshooting Tests

### Common Issues

**Issue: Import errors**
```bash
# Solution: Add parent directory to Python path
export PYTHONPATH="${PYTHONPATH}:$(pwd)/lambda"
```

**Issue: AWS credentials not found**
```bash
# Solution: Configure AWS CLI
aws configure
# Or set environment variables
export AWS_ACCESS_KEY_ID=your-key
export AWS_SECRET_ACCESS_KEY=your-secret
export AWS_DEFAULT_REGION=ap-southeast-1
```

**Issue: DynamoDB table not found**
```bash
# Solution: Deploy infrastructure first
cd infrastructure
cdk deploy RDSDashboard-Data-prod
```

**Issue: Lambda timeout**
```bash
# Solution: Increase timeout in compute-stack.ts
timeout: cdk.Duration.minutes(15)  // Increase if needed
```

---

## 10. Continuous Testing

### GitHub Actions (Example)

Create `.github/workflows/test.yml`:

```yaml
name: Test RDS Dashboard

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v2
    
    - name: Set up Python
      uses: actions/setup-python@v2
      with:
        python-version: '3.11'
    
    - name: Install dependencies
      run: |
        cd lambda
        pip install -r requirements.txt
        pip install pytest pytest-cov moto
    
    - name: Run unit tests
      run: |
        cd lambda
        pytest tests/ -v --cov=.
    
    - name: Upload coverage
      uses: codecov/codecov-action@v2
```

---

## Testing Checklist

Before deployment:
- [ ] Unit tests pass locally
- [ ] Python syntax check passes
- [ ] TypeScript compiles without errors
- [ ] CDK synth succeeds
- [ ] Integration tests pass in test environment
- [ ] Performance is acceptable (< 5 min for 50 instances)
- [ ] Cache hit rate > 70% on second run
- [ ] CloudWatch metrics are published
- [ ] Logs are structured and readable
- [ ] No sensitive data in logs

After deployment:
- [ ] Discovery Lambda runs successfully
- [ ] Instances appear in DynamoDB
- [ ] Health Monitor Lambda runs successfully
- [ ] Metrics are cached in DynamoDB
- [ ] CloudWatch metrics are visible
- [ ] SNS notifications work (if triggered)
- [ ] Cross-account access works
- [ ] No errors in CloudWatch Logs

---

## Quick Test Commands

```bash
# Test everything quickly
cd rds-operations-dashboard

# 1. Syntax check
python -m py_compile lambda/**/*.py

# 2. CDK check
cd infrastructure && npm run build && cdk synth

# 3. Deploy to test
cdk deploy --all -c environment=test

# 4. Test discovery
aws lambda invoke --function-name rds-discovery-test --payload '{}' response.json

# 5. Test health monitor
aws lambda invoke --function-name rds-health-monitor-test --payload '{}' response.json

# 6. Check results
cat response.json | jq .
aws dynamodb scan --table-name rds-inventory-test --max-items 5
aws cloudwatch list-metrics --namespace DBMRDSDashboard
```

---

## Next Steps

1. **Start with Unit Tests** - Fastest feedback loop
2. **Deploy to Test Environment** - Validate with real AWS services
3. **Run Integration Tests** - Verify end-to-end workflows
4. **Monitor Performance** - Ensure meets requirements
5. **Deploy to Production** - After all tests pass

For questions or issues, check CloudWatch Logs first!
