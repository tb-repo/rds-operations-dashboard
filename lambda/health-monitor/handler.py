"""
RDS Health Monitor Service Lambda Handler

Generated by: claude-3.5-sonnet
Timestamp: 2025-11-12T19:00:00Z
Version: 1.0.0
Policy Version: v1.0.0
Traceability: REQ-2.1, REQ-8.1, REQ-8.2 → DESIGN-001 → TASK-3
Review Status: Pending
Risk Level: Level 2

Purpose: Monitor RDS instance health by collecting CloudWatch metrics.
Implements cache-first strategy to minimize API calls and data transfer costs.

Design Decisions:
- Cache-first: Check DynamoDB before querying CloudWatch (70% cache hit target)
- Batch metric requests: Single API call for multiple metrics
- Optimized intervals: 5-min for critical metrics, 1-hour for standard
- Parallel processing: Monitor multiple instances concurrently
"""

import json
import os
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
from concurrent.futures import ThreadPoolExecutor, as_completed

import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..'))

from shared import AWSClients, StructuredLogger, Config, log_execution
from cache_manager import MetricsCacheManager
from alerting import AlertManager

# Initialize logger
logger = StructuredLogger('health-monitor-service')


def lambda_handler(event, context):
    """
    Lambda handler for RDS health monitoring.
    
    Args:
        event: EventBridge scheduled event or manual invocation
        context: Lambda context
    
    Returns:
        dict: Health monitoring results summary
    
    Requirements: REQ-2.1 (health monitoring), REQ-8.1 (cost optimization)
    """
    correlation_id = context.aws_request_id if context else 'local-test'
    logger.set_correlation_id(correlation_id)
    
    logger.info('Health monitor service started',
                function_name=context.function_name if context else 'local',
                aws_request_id=correlation_id)
    
    try:
        # Load configuration
        config = Config.load()
        
        # Get active instances from DynamoDB
        instances = get_active_instances(config)
        
        if not instances:
            logger.warn('No active instances found')
            return {
                'statusCode': 200,
                'body': json.dumps({'message': 'No instances to monitor'})
            }
        
        logger.info(f'Monitoring {len(instances)} instances',
                   instance_count=len(instances))
        
        # Initialize cache manager and alert manager
        cache_manager = MetricsCacheManager(config)
        alert_manager = AlertManager(config)
        
        # Monitor health for all instances
        health_results = monitor_all_instances(instances, config, cache_manager, alert_manager)
        
        # Publish cache metrics to CloudWatch
        cache_manager.publish_cache_metrics()
        
        # Log summary
        logger.info('Health monitor service completed',
                   instances_monitored=health_results['instances_monitored'],
                   cache_hit_rate=health_results['cache_hit_rate'],
                   cache_hits=health_results['cache_hits'],
                   cache_misses=health_results['cache_misses'],
                   alerts_generated=health_results['alerts_generated'])
        
        return {
            'statusCode': 200,
            'body': json.dumps(health_results, default=str)
        }
        
    except Exception as e:
        logger.error('Health monitor service failed',
                    error_type=type(e).__name__,
                    error_message=str(e))
        raise


def get_active_instances(config: Any) -> List[Dict[str, Any]]:
    """
    Get list of active RDS instances from DynamoDB inventory.
    
    Args:
        config: Application configuration
    
    Returns:
        list: Active RDS instances
    """
    try:
        dynamodb = AWSClients.get_dynamodb_resource()
        table = dynamodb.Table(config.dynamodb.inventory_table)
        
        # Scan for active instances (not deleted)
        response = table.scan(
            FilterExpression='attribute_not_exists(is_deleted) OR is_deleted = :false',
            ExpressionAttributeValues={':false': False}
        )
        
        instances = response.get('Items', [])
        
        logger.info(f'Retrieved {len(instances)} active instances from inventory')
        
        return instances
        
    except Exception as e:
        logger.error('Failed to get active instances',
                    error=str(e))
        return []


@log_execution(logger)
def monitor_all_instances(
    instances: List[Dict[str, Any]],
    config: Any,
    cache_manager: MetricsCacheManager,
    alert_manager: AlertManager
) -> Dict[str, Any]:
    """
    Monitor health for all instances with parallel processing.
    
    Args:
        instances: List of RDS instances
        config: Application configuration
    
    Returns:
        dict: Monitoring results summary
    """
    cache_hits = 0
    cache_misses = 0
    alerts_generated = 0
    errors = 0
    
    # Use ThreadPoolExecutor for parallel monitoring
    max_workers = min(10, len(instances))  # Max 10 concurrent
    
    all_alerts = []
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit monitoring tasks
        future_to_instance = {
            executor.submit(monitor_instance, instance, config, cache_manager, alert_manager): instance
            for instance in instances
        }
        
        # Collect results
        for future in as_completed(future_to_instance):
            instance = future_to_instance[future]
            try:
                result = future.result()
                if result['alerts']:
                    all_alerts.extend(result['alerts'])
                    alerts_generated += len(result['alerts'])
                
            except Exception as e:
                logger.error('Failed to monitor instance',
                           instance_id=instance.get('instance_id'),
                           error=str(e))
                errors += 1
    
    # Store all alerts in DynamoDB
    if all_alerts:
        alert_manager.store_alerts(all_alerts)
        
        # Send notifications for critical alerts
        notifications_sent = alert_manager.send_notifications(all_alerts)
        logger.info(f'Sent {notifications_sent} critical alert notifications')
    
    # Get cache statistics from cache manager
    cache_stats = cache_manager.get_statistics()
    
    return {
        'instances_monitored': len(instances) - errors,
        'cache_hits': cache_stats['cache_hits'],
        'cache_misses': cache_stats['cache_misses'],
        'cache_hit_rate': cache_stats['cache_hit_rate'],
        'alerts_generated': alerts_generated,
        'errors': errors,
        'timestamp': datetime.utcnow().isoformat() + 'Z'
    }


def monitor_instance(
    instance: Dict[str, Any],
    config: Any,
    cache_manager: MetricsCacheManager,
    alert_manager: AlertManager
) -> Dict[str, Any]:
    """
    Monitor health metrics for a single RDS instance.
    
    Args:
        instance: RDS instance data
        config: Application configuration
    
    Returns:
        dict: Monitoring results for this instance
    
    Requirements: REQ-2.1 (health monitoring), REQ-8.2 (caching)
    """
    instance_id = instance['instance_id']
    account_id = instance['account_id']
    region = instance['region']
    
    logger.debug(f'Monitoring instance: {instance_id}',
                instance_id=instance_id,
                account_id=account_id,
                region=region)
    
    # Define metrics to collect
    metrics_to_collect = get_metrics_for_instance(instance)
    
    # Collect metrics (cache-first) and build metrics dictionary
    current_metrics = {}
    
    for metric_config in metrics_to_collect:
        cached = cache_manager.get(
            instance_id=instance_id,
            metric_name=metric_config['name'],
            period=metric_config['period']
        )
        
        if cached:
            current_metrics[metric_config['name']] = cached['value']
        else:
            # Fetch from CloudWatch and cache
            value = fetch_and_cache_metric(
                instance=instance,
                metric_config=metric_config,
                cache_manager=cache_manager,
                config=config
            )
            if value is not None:
                current_metrics[metric_config['name']] = value
    
    # Evaluate metrics against thresholds
    alerts = alert_manager.evaluate_metrics(instance, current_metrics)
    
    return {
        'instance_id': instance_id,
        'alerts': alerts
    }


def get_metrics_for_instance(instance: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    Get list of metrics to collect based on instance type and engine.
    
    Args:
        instance: RDS instance data
    
    Returns:
        list: Metric configurations
    
    Requirements: REQ-2.1 (comprehensive monitoring)
    """
    # Critical metrics (5-minute intervals)
    critical_metrics = [
        {'name': 'CPUUtilization', 'period': 300, 'stat': 'Average'},
        {'name': 'DatabaseConnections', 'period': 300, 'stat': 'Average'},
        {'name': 'FreeStorageSpace', 'period': 300, 'stat': 'Average'},
    ]
    
    # Standard metrics (1-hour intervals)
    standard_metrics = [
        {'name': 'ReadIOPS', 'period': 3600, 'stat': 'Average'},
        {'name': 'WriteIOPS', 'period': 3600, 'stat': 'Average'},
        {'name': 'ReadLatency', 'period': 3600, 'stat': 'Average'},
        {'name': 'WriteLatency', 'period': 3600, 'stat': 'Average'},
        {'name': 'DiskQueueDepth', 'period': 3600, 'stat': 'Average'},
    ]
    
    # Combine all metrics
    all_metrics = critical_metrics + standard_metrics
    
    return all_metrics


def get_cached_metric(
    instance_id: str,
    metric_name: str,
    period: int,
    config: Any
) -> Optional[Dict[str, Any]]:
    """
    Get metric from DynamoDB cache if available and fresh.
    
    Args:
        instance_id: RDS instance identifier
        metric_name: CloudWatch metric name
        period: Metric period in seconds
        config: Application configuration
    
    Returns:
        dict: Cached metric data or None
    
    Requirements: REQ-8.2 (caching), REQ-8.3 (TTL)
    """
    try:
        dynamodb = AWSClients.get_dynamodb_resource()
        table = dynamodb.Table(config.dynamodb.metrics_cache_table)
        
        # Cache key format: instance_id#metric_name#period
        cache_key = f"{instance_id}#{metric_name}#{period}"
        
        response = table.get_item(Key={'cache_key': cache_key})
        
        if 'Item' in response:
            item = response['Item']
            
            # Check if cache is still fresh (TTL not expired)
            ttl = item.get('ttl', 0)
            current_time = int(datetime.utcnow().timestamp())
            
            if ttl > current_time:
                logger.debug(f'Cache hit: {cache_key}')
                return item
            else:
                logger.debug(f'Cache expired: {cache_key}')
                return None
        
        logger.debug(f'Cache miss: {cache_key}')
        return None
        
    except Exception as e:
        logger.error('Failed to get cached metric',
                    cache_key=cache_key,
                    error=str(e))
        return None


def fetch_and_cache_metric(
    instance: Dict[str, Any],
    metric_config: Dict[str, Any],
    cache_manager: MetricsCacheManager,
    config: Any
) -> Optional[float]:
    """
    Fetch metric from CloudWatch and store in cache.
    
    Args:
        instance: RDS instance data
        metric_config: Metric configuration
        config: Application configuration
    
    Requirements: REQ-2.1 (CloudWatch metrics), REQ-8.2 (caching)
    """
    instance_id = instance['instance_id']
    account_id = instance['account_id']
    region = instance['region']
    metric_name = metric_config['name']
    period = metric_config['period']
    stat = metric_config['stat']
    
    try:
        # Get CloudWatch client (cross-account if needed)
        if account_id == os.environ.get('AWS_ACCOUNT_ID'):
            cw_client = AWSClients.get_cloudwatch_client(region=region)
        else:
            cw_client = AWSClients.get_cloudwatch_client(
                region=region,
                account_id=account_id,
                role_name=config.cross_account.role_name,
                external_id=config.cross_account.external_id
            )
        
        # Query CloudWatch for metric
        end_time = datetime.utcnow()
        start_time = end_time - timedelta(seconds=period * 2)  # Get 2 periods
        
        response = cw_client.get_metric_statistics(
            Namespace='AWS/RDS',
            MetricName=metric_name,
            Dimensions=[
                {'Name': 'DBInstanceIdentifier', 'Value': instance_id}
            ],
            StartTime=start_time,
            EndTime=end_time,
            Period=period,
            Statistics=[stat]
        )
        
        datapoints = response.get('Datapoints', [])
        
        if datapoints:
            # Sort by timestamp and get latest
            datapoints.sort(key=lambda x: x['Timestamp'], reverse=True)
            latest = datapoints[0]
            value = latest.get(stat, 0)
            
            # Store in cache using cache manager
            cache_manager.put(
                instance_id=instance_id,
                metric_name=metric_name,
                period=period,
                value=value,
                timestamp=latest['Timestamp'],
                unit=latest.get('Unit', 'None')
            )
            
            logger.debug(f'Fetched and cached metric: {metric_name}',
                        instance_id=instance_id,
                        value=value)
            
            return value
        
        return None
        
    except Exception as e:
        logger.error('Failed to fetch metric from CloudWatch',
                    instance_id=instance_id,
                    metric_name=metric_name,
                    error=str(e))
        return None


def store_metric_in_cache(
    instance_id: str,
    metric_name: str,
    period: int,
    value: float,
    timestamp: datetime,
    unit: str,
    config: Any
) -> None:
    """
    Store metric in DynamoDB cache with TTL.
    
    Args:
        instance_id: RDS instance identifier
        metric_name: CloudWatch metric name
        period: Metric period
        value: Metric value
        timestamp: Metric timestamp
        unit: Metric unit
        config: Application configuration
    
    Requirements: REQ-8.2 (caching), REQ-8.3 (TTL)
    """
    try:
        dynamodb = AWSClients.get_dynamodb_resource()
        table = dynamodb.Table(config.dynamodb.metrics_cache_table)
        
        # Cache key
        cache_key = f"{instance_id}#{metric_name}#{period}"
        
        # Calculate TTL (current time + cache TTL)
        ttl = int(datetime.utcnow().timestamp()) + config.cache.metrics_ttl_seconds
        
        # Store in DynamoDB
        table.put_item(
            Item={
                'cache_key': cache_key,
                'instance_id': instance_id,
                'metric_name': metric_name,
                'period': period,
                'value': float(value),
                'timestamp': timestamp.isoformat(),
                'unit': unit,
                'ttl': ttl,
                'cached_at': datetime.utcnow().isoformat() + 'Z'
            }
        )
        
        logger.debug(f'Stored metric in cache: {cache_key}')
        
    except Exception as e:
        logger.error('Failed to store metric in cache',
                    cache_key=cache_key,
                    error=str(e))


# For local testing
if __name__ == '__main__':
    class MockContext:
        request_id = 'test-request-123'
        function_name = 'rds-health-monitor-test'
    
    # Set environment variables
    os.environ['INVENTORY_TABLE'] = 'rds-inventory-test'
    os.environ['METRICS_CACHE_TABLE'] = 'metrics-cache-test'
    os.environ['HEALTH_ALERTS_TABLE'] = 'health-alerts-test'
    os.environ['AUDIT_LOG_TABLE'] = 'audit-log-test'
    os.environ['DATA_BUCKET'] = 'rds-dashboard-data-test'
    os.environ['EXTERNAL_ID'] = 'test-external-id'
    os.environ['TARGET_ACCOUNTS'] = '["123456789012"]'
    os.environ['SNS_TOPIC_ARN'] = 'arn:aws:sns:ap-southeast-1:123456789012:test'
    
    result = lambda_handler({}, MockContext())
    print(json.dumps(result, indent=2))
